#!/usr/bin/env python2

###########################################################################
#
#  Copyright 2011-2013 The University of North Carolina at Chapel Hill
#  and Michigan State University. All rights reserved.
#
#  Licensed under the MADAI Software License. You may obtain a copy of
#  this license at
#
#      https://madai-public.cs.unc.edu/visualization/software-license/
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#
###########################################################################

import os
import sys
import copy
import math
import gzip

class Correlations(object):
    def __init__(self, number_of_parameters, number_of_observables):
        self.number_of_parameters = number_of_parameters
        self.number_of_observables = number_of_observables
        self.samples = 0
        self.po_squares = [ [0.0]*number_of_observables for i in range(number_of_parameters) ]
        self.pp_squares = [ [0.0]*number_of_parameters for i in range(number_of_parameters) ]
        self.parameter_sums = [0.0]*number_of_parameters
        self.observable_sums = [0.0]*number_of_observables

        # De represents the derivative with respect to the error on the observables
        self.De_samples = [0.0]*number_of_observables
        self.De_parameter_sums = []
        self.De_pp_squares = []
        for i in range(number_of_observables):
            self.De_parameter_sums.append(copy.deepcopy(self.parameter_sums))
            self.De_pp_squares.append(copy.deepcopy(self.pp_squares))

        # Dv represents the derivative with respect to the value of the observable
        self.Dv_samples = [0.0]*number_of_observables
        self.Dv_parameter_sums = []
        self.Dv_pp_squares = []
        for i in range(number_of_observables):
            self.Dv_parameter_sums.append(copy.deepcopy(self.parameter_sums))
            self.Dv_pp_squares.append(copy.deepcopy(self.pp_squares))


    def add_sample(self, parameters, observables, loglikelihood, value_gradient, error_gradient):
        self.samples += 1
        for i, parameter in enumerate(parameters):
            self.parameter_sums[i] += parameter
            for j in range(i+1):
                self.pp_squares[i][j] += parameter*parameters[j]
            for j, observable in enumerate(observables):
                self.po_squares[i][j] += parameter*observable
        for k, observable in enumerate(observables):
            self.observable_sums[k] += observable
            self.Dv_samples[k] += value_gradient[k]
            self.De_samples[k] += error_gradient[k]
            for i, parameter in enumerate(parameters):
                self.Dv_parameter_sums[k][i] += parameter*value_gradient[k]
                self.De_parameter_sums[k][i] += parameter*error_gradient[k]
                for j in range(i+1):
                    self.Dv_pp_squares[k][i][j] += parameter*parameters[j]*value_gradient[k]
                    self.De_pp_squares[k][i][j] += parameter*parameters[j]*error_gradient[k]

    def parameter_mean(self, i):
        return self.parameter_sums[i]/float(self.samples)

    def observable_mean(self, i):
        return self.observable_sums[i]/float(self.samples)

    def parameter_parameter_covariance(self, i, j):
        if j > i:
            i, j = j, i
        N = float(self.samples)
        return (self.pp_squares[i][j] - (self.parameter_sums[i]*self.parameter_sums[j]/N))/N

    def parameter_variance(self, i):
        return self.parameter_parameter_covariance(i, i)

    def parameter_observable_covariance(self, i, j):
        N = float(self.samples)
        return (self.po_squares[i][j] - (self.parameter_sums[i]*self.observable_sums[j]/N))/N


    def De_parameter_parameter_covariance(self, k, i, j):
        N = float(self.samples)
        De_covariance = self.De_pp_squares[k][i][j]/N
        De_covariance -= self.pp_squares[i][j]*self.De_samples[k]/(N*N)

        De_covariance -= self.parameter_mean(i)*self.De_parameter_sums[k][j]/N
        De_covariance += self.parameter_mean(i)*self.parameter_mean(j)*self.De_samples[k]/N

        De_covariance -= self.parameter_mean(j)*self.De_parameter_sums[k][i]/N
        De_covariance += self.parameter_mean(j)*self.parameter_mean(i)*self.De_samples[k]/N

        return De_covariance/self.parameter_parameter_covariance(i, j)
    
    def De_parameter_parameter_root_covariance(self, k, i, j):
        return self.De_parameter_parameter_covariance(k, i, j)/2.0

    def De_parameter_standard_deviation(self, k, i):
        return self.De_parameter_parameter_root_covariance(k, i, i)

    def parameter_response_to_value(self, observable_index, parameter_index):
        Dv_parameter_mean = self.Dv_parameter_sums[observable_index][parameter_index]/self.samples
        parameter_mean = self.parameter_mean(parameter_index)
        Dv_mean = self.Dv_samples[observable_index]/self.samples

        return Dv_parameter_mean - parameter_mean*Dv_mean

    def parameter_response_to_error(self, observable_index, parameter_index):
        De_parameter_mean = self.De_parameter_sums[observable_index][parameter_index]/self.samples
        parameter_mean = self.parameter_mean(parameter_index)
        De_mean = self.De_samples[observable_index]/self.samples

        return De_parameter_mean - parameter_mean*De_mean

    def resolving_power(self, k , i, j):
        if i == j:
            return self.De_parameter_standard_deviation(k, i)

        variance = (self.parameter_variance(i), self.parameter_variance(j))
        standard_deviation = (math.sqrt(variance[0]), math.sqrt(variance[1]))
        covariance = self.parameter_parameter_covariance(i, j)

        # there's an arbitrary scale if the two parameters have different units
        # here we scale each by their standard deviation
        scale = (standard_deviation[0], standard_deviation[1])
        standard_deviation = (standard_deviation[0]/scale[0], standard_deviation[1]/scale[1])
        variance = (standard_deviation[0]**2, standard_deviation[1]**2)
        covariance /= scale[0]*scale[1]

        # pick pi/4 if they're the same, as is the case when scaled by the standard deviation
        major_theta = math.pi/4.0
        if variance[0] != variance[1]:
            major_theta = -0.5*math.atan(2.0*covariance/(variance[1]-variance[0]))
        minor_theta = major_theta + math.pi/2.0
        major_cos, major_sin = math.cos(major_theta), math.sin(major_theta)
        minor_cos, minor_sin = math.cos(minor_theta), math.sin(minor_theta)
        
        major_variance = (major_cos**2)*variance[0]
        major_variance += (major_sin**2)*variance[1]
        major_variance += 2.0*major_cos*major_sin*covariance

        minor_variance = (minor_cos**2)*variance[0]
        minor_variance += (minor_sin**2)*variance[1]
        minor_variance += 2.0*minor_cos*minor_sin*covariance

        #It's arbitrary which is larger so swap them if necessary
        if minor_variance > major_variance:
            minor_variance, major_variance = major_variance, minor_variance
            minor_theta, major_theta = major_theta, minor_theta
            minor_cos, major_cos = major_cos, minor_cos
            minor_sin, major_sin = major_sin, minor_sin
        major_standard_deviation = math.sqrt(major_variance)
        minor_standard_deviation = math.sqrt(minor_variance)

        De_standard_deviation = (self.De_parameter_standard_deviation(k, i), self.De_parameter_standard_deviation(k, j))

        De_major_variance = (major_cos**2)*2.0*variance[0]*De_standard_deviation[0]
        De_major_variance += (major_sin**2)*2.0*variance[1]*De_standard_deviation[1]
        De_major_variance += 2.0*major_cos*major_sin*covariance*self.De_parameter_parameter_covariance(k, i, j)

        De_minor_variance = (minor_cos**2)*2.0*variance[0]*De_standard_deviation[0]
        De_minor_variance += (minor_sin**2)*2.0*variance[1]*De_standard_deviation[1]
        De_minor_variance += 2.0*minor_cos*minor_sin*covariance*self.De_parameter_parameter_covariance(k, i, j)

        De_major_resolving_power = De_major_variance/(2.0*major_variance)
        De_minor_resolving_power = De_minor_variance/(2.0*minor_variance)

        if i < j:
            return De_major_resolving_power
        else:
            return De_minor_resolving_power

class TraceFileException(Exception):
    pass

def read_trace_file(filename):
    if not os.path.isfile(filename):
        raise TraceFileException('Trace file ' + filename + ' does not exist.')
    f = open(filename, 'rb')
    if f.read(1) == b'"':
        f.close()
        f = open(filename,'r')
    else:
        f.close()
        f = gzip.open(filename,'r')
    header = [name[1:-1] for name in f.readline()[:-1].split(',')]
    if header[-1] == 'LogLikelihood':
        TraceFileException('There are no log likelihood gradients in the trace file.')
    if 'LogLikelihood' not in header:
        TraceFileException('There is no log likelihood in the trace file.')
    loglikelihood_index = header.index('LogLikelihood')
    number_of_observables = 0
    for title in header[loglikelihood_index+1:]:
        if '*dLL/dsigma_' in title:
            break
        number_of_observables += 1
    number_of_parameters = loglikelihood_index - number_of_observables
    parameter_names = header[:number_of_parameters]
    observable_names = header[number_of_parameters:loglikelihood_index]
    correlations = Correlations(number_of_parameters, number_of_observables)
    for line in f:
        line = map(float, line.split(','))
        parameters = line[:number_of_parameters]
        observables = line[number_of_parameters:loglikelihood_index]
        loglikelihood = line[loglikelihood_index]
        value_gradient = line[loglikelihood_index+1:loglikelihood_index+1+number_of_observables]
        error_gradient = line[loglikelihood_index+1+number_of_observables:loglikelihood_index+1+2*number_of_observables]
        correlations.add_sample(parameters, observables, loglikelihood, value_gradient, error_gradient)

    row_format ="{:>30}" * (number_of_parameters + 1)
    for k, observable in enumerate(observable_names):
        print "Resolving power for the observable " + observable
        print row_format.format("", *parameter_names)
        for i, parameter in enumerate(parameter_names):
            resolving_powers = []
            for j in range(number_of_parameters):
                resolving_powers.append(correlations.resolving_power(k, i, j))
            print row_format.format(parameter, *resolving_powers)

    print "Parameter response d<parameter>/dobservable:"
    print row_format.format("", *parameter_names)
    for k, observable in enumerate(observable_names):
        parameter_responses = []
        for i, parameter in enumerate(parameter_names):
            parameter_responses.append(correlations.parameter_response_to_value(k, i))
        print row_format.format(observable, *parameter_responses)
    return correlations

    print "Parameter response d<parameter>/dsigma_observable:"
    print row_format.format("", *parameter_names)
    for k, observable in enumerate(observable_names):
        parameter_responses = []
        for i, parameter in enumerate(parameter_names):
            parameter_responses.append(correlations.parameter_response_to_error(k, i))
        print row_format.format(observable, *parameter_responses)
    return correlations



if __name__ == '__main__':
    if len(sys.argv) != 2:
        print 'Usage: ' + sys.argv[0] + ' trace_file.csv'
        exit
    try:
        correlations = read_trace_file(sys.argv[1])
    except TraceFileException as e:
        print 'Fatal exception: ' + e

