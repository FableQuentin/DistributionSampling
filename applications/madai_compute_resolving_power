#!/usr/bin/env python2

###########################################################################
#
#  Copyright 2011-2013 The University of North Carolina at Chapel Hill
#  and Michigan State University. All rights reserved.
#
#  Licensed under the MADAI Software License. You may obtain a copy of
#  this license at
#
#      https://madai-public.cs.unc.edu/visualization/software-license/
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#
###########################################################################

import os
import sys
import copy
import math
import gzip
import base64
import matplotlib as mpl
import matplotlib.cm as cm

#Global settings for now:
N_SIGMA = 4
N_BINS = 50

class Table(object):
    def __init__(self, html_class, rows, columns, row_type, column_type):
        valid_types = ['parameter', 'observable', 'other']
        assert row_type in valid_types
        assert column_type in valid_types

        valid_types = ['data_table', 'image_table']
        assert html_class in valid_types

        self.html_class = html_class
        self.rows = rows
        self.columns = columns
        self.row_type = row_type
        self.column_type = column_type

class DataTable(Table):
    def __init__(self, rows, columns, row_type, column_type, data, cmap = cm.seismic):
        Table.__init__(self, html_class = 'data_table', rows = rows, columns = columns, \
                       row_type = row_type, column_type = column_type)

        vmax = max(map(max, data))
        vmin = min(map(min, data))
        vmax = max( [abs(vmin), abs(vmax)] )
        vmin = - vmax
        norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)
        scalar_map = cm.ScalarMappable(norm=norm, cmap=cmap)

        self.colors, self.data = [], []
        for i in range(len(rows)):
            c, d = [], []
            for j in range(len(columns)):
                c.append('rgb' + str(tuple(map(lambda x: int(256*x), scalar_map.to_rgba(data[i][j]))[:3])))
                d.append(data[i][j])
            self.colors.append(c)
            self.data.append(d)

        self.csv = ''
        for i in range(-1,len(rows)):
            self.csv += '"'
            if i >= 0:
                self.csv += rows[i]
            self.csv += '"'

            for j in range(len(columns)):
                if i < 0:
                    self.csv += ',"' + columns[j] + '"'
                else:
                    self.csv += ',' + str(self.data[i][j])
            self.csv += '\n'
        self.csv = 'data:test/plain;base64,\n' + base64.b64encode(self.csv)

class ImageTable(Table):
    def __init__(self, rows, columns, row_type, column_type, data):
        Table.__init__(self, html_class = 'image_table', rows = rows, columns = columns, \
                       row_type = row_type, column_type = column_type)

        self.images = []
        for i in range(len(rows)):
            d = []
            for j in range(len(columns)):
                encoded_image = 'data:image/svg+xml;base64,\n'
                encoded_image += base64.b64encode(data[i][j])
                d.append(encoded_image)
            self.images.append(d)

class Correlations(object):
    def __init__(self, number_of_parameters, number_of_observables):
        self.number_of_parameters = number_of_parameters
        self.number_of_observables = number_of_observables
        self.samples = 0
        self.po_squares = [ [0.0]*number_of_observables for i in range(number_of_parameters) ]
        self.pp_squares = [ [0.0]*number_of_parameters for i in range(number_of_parameters) ]
        self.parameter_sums = [0.0]*number_of_parameters
        self.observable_sums = [0.0]*number_of_observables
        self.parameter_squares = [0.0]*number_of_parameters
        self.observable_squares = [0.0]*number_of_observables
        self.loglikelihood_sums = [0.0]*number_of_observables

        # De represents the derivative with respect to the error on the observables
        self.De_samples = [0.0]*number_of_observables
        self.De_loglikelihood_sums = [0.0]*number_of_observables
        self.De_parameter_sums = []
        self.De_pp_squares = []
        for i in range(number_of_observables):
            self.De_parameter_sums.append(copy.deepcopy(self.parameter_sums))
            self.De_pp_squares.append(copy.deepcopy(self.pp_squares))

        # Dv represents the derivative with respect to the value of the observable
        self.Dv_samples = [0.0]*number_of_observables
        self.Dv_loglikelihood_sums = [0.0]*number_of_observables
        self.Dv_parameter_sums = []
        self.Dv_pp_squares = []
        for i in range(number_of_observables):
            self.Dv_parameter_sums.append(copy.deepcopy(self.parameter_sums))
            self.Dv_pp_squares.append(copy.deepcopy(self.pp_squares))


    def add_sample(self, parameters, observables, loglikelihood, value_gradient, error_gradient):
        self.samples += 1
        for i, parameter in enumerate(parameters):
            self.parameter_sums[i] += parameter
            self.parameter_squares[i] += parameter**2
            for j in range(i+1):
                self.pp_squares[i][j] += parameter*parameters[j]
            for j, observable in enumerate(observables):
                self.po_squares[i][j] += parameter*observable
        for k, observable in enumerate(observables):
            self.observable_sums[k] += observable
            self.observable_squares[k] += observable**2
            self.Dv_samples[k] += value_gradient[k]
            self.De_samples[k] += error_gradient[k]
            self.Dv_loglikelihood_sums[k] += value_gradient[k]*loglikelihood
            self.De_loglikelihood_sums[k] += error_gradient[k]*loglikelihood
            self.loglikelihood_sums[k] += loglikelihood
            for i, parameter in enumerate(parameters):
                self.Dv_parameter_sums[k][i] += parameter*value_gradient[k]
                self.De_parameter_sums[k][i] += parameter*error_gradient[k]
                for j in range(i+1):
                    self.Dv_pp_squares[k][i][j] += parameter*parameters[j]*value_gradient[k]
                    self.De_pp_squares[k][i][j] += parameter*parameters[j]*error_gradient[k]

    def parameter_mean(self, i):
        return self.parameter_sums[i]/float(self.samples)

    def parameter_standard_deviation(self, i):
        x2 = self.parameter_squares[i]/float(self.samples)
        x = self.parameter_mean(i)
        return x2 - x*x

    def observable_mean(self, i):
        return self.observable_sums[i]/float(self.samples)

    def observable_standard_deviation(self, i):
        y2 = self.observable_squares[i]/float(self.samples)
        y = self.observable_mean(i)
        return y2 - y*y

    def parameter_parameter_covariance(self, i, j):
        if j > i:
            i, j = j, i
        N = float(self.samples)
        return (self.pp_squares[i][j] - (self.parameter_sums[i]*self.parameter_sums[j]/N))/N

    def parameter_variance(self, i):
        return self.parameter_parameter_covariance(i, i)

    def parameter_observable_covariance(self, i, j):
        N = float(self.samples)
        return (self.po_squares[i][j] - (self.parameter_sums[i]*self.observable_sums[j]/N))/N


    def De_parameter_parameter_covariance(self, k, i, j):
        N = float(self.samples)
        De_covariance = self.De_pp_squares[k][i][j]/N
        De_covariance -= self.pp_squares[i][j]*self.De_samples[k]/(N*N)

        De_covariance -= self.parameter_mean(i)*self.De_parameter_sums[k][j]/N
        De_covariance += self.parameter_mean(i)*self.parameter_mean(j)*self.De_samples[k]/N

        De_covariance -= self.parameter_mean(j)*self.De_parameter_sums[k][i]/N
        De_covariance += self.parameter_mean(j)*self.parameter_mean(i)*self.De_samples[k]/N

        return De_covariance/self.parameter_parameter_covariance(i, j)
    
    def De_parameter_parameter_root_covariance(self, k, i, j):
        return self.De_parameter_parameter_covariance(k, i, j)/2.0

    def De_parameter_standard_deviation(self, k, i):
        return self.De_parameter_parameter_root_covariance(k, i, i)

    def parameter_response_to_value(self, observable_index, parameter_index):
        Dv_parameter_mean = self.Dv_parameter_sums[observable_index][parameter_index]/float(self.samples)
        parameter_mean = self.parameter_mean(parameter_index)
        Dv_mean = self.Dv_samples[observable_index]/float(self.samples)

        return Dv_parameter_mean - parameter_mean*Dv_mean

    def parameter_response_to_error(self, observable_index, parameter_index):
        De_parameter_mean = self.De_parameter_sums[observable_index][parameter_index]/float(self.samples)
        parameter_mean = self.parameter_mean(parameter_index)
        De_mean = self.De_samples[observable_index]/float(self.samples)

        return De_parameter_mean - parameter_mean*De_mean

    def loglikelihood_response_to_value(self, observable_index):
        Dv_loglikelihood_mean = self.Dv_loglikelihood_sums[observable_index]/float(self.samples)
        loglikelihood_mean = self.loglikelihood_sums[observable_index]/float(self.samples)
        Dv_mean = self.Dv_samples[observable_index]/float(self.samples)

        return Dv_loglikelihood_mean - loglikelihood_mean*Dv_mean

    def resolving_power(self, k , i, j):
        if i == j:
            return self.De_parameter_standard_deviation(k, i)

        variance = (self.parameter_variance(i), self.parameter_variance(j))
        standard_deviation = (math.sqrt(variance[0]), math.sqrt(variance[1]))
        covariance = self.parameter_parameter_covariance(i, j)

        # there's an arbitrary scale if the two parameters have different units
        # here we scale each by their standard deviation
        scale = (standard_deviation[0], standard_deviation[1])
        standard_deviation = (standard_deviation[0]/scale[0], standard_deviation[1]/scale[1])
        variance = (standard_deviation[0]**2, standard_deviation[1]**2)
        covariance /= scale[0]*scale[1]

        # pick pi/4 if they're the same, as is the case when scaled by the standard deviation
        major_theta = math.pi/4.0
        if variance[0] != variance[1]:
            major_theta = -0.5*math.atan(2.0*covariance/(variance[1]-variance[0]))
        minor_theta = major_theta + math.pi/2.0
        major_cos, major_sin = math.cos(major_theta), math.sin(major_theta)
        minor_cos, minor_sin = math.cos(minor_theta), math.sin(minor_theta)
        
        major_variance = (major_cos**2)*variance[0]
        major_variance += (major_sin**2)*variance[1]
        major_variance += 2.0*major_cos*major_sin*covariance

        minor_variance = (minor_cos**2)*variance[0]
        minor_variance += (minor_sin**2)*variance[1]
        minor_variance += 2.0*minor_cos*minor_sin*covariance

        #It's arbitrary which is larger so swap them if necessary
        if minor_variance > major_variance:
            minor_variance, major_variance = major_variance, minor_variance
            minor_theta, major_theta = major_theta, minor_theta
            minor_cos, major_cos = major_cos, minor_cos
            minor_sin, major_sin = major_sin, minor_sin
        major_standard_deviation = math.sqrt(major_variance)
        minor_standard_deviation = math.sqrt(minor_variance)

        De_standard_deviation = (self.De_parameter_standard_deviation(k, i), self.De_parameter_standard_deviation(k, j))

        De_major_variance = (major_cos**2)*2.0*variance[0]*De_standard_deviation[0]
        De_major_variance += (major_sin**2)*2.0*variance[1]*De_standard_deviation[1]
        De_major_variance += 2.0*major_cos*major_sin*covariance*self.De_parameter_parameter_covariance(k, i, j)

        De_minor_variance = (minor_cos**2)*2.0*variance[0]*De_standard_deviation[0]
        De_minor_variance += (minor_sin**2)*2.0*variance[1]*De_standard_deviation[1]
        De_minor_variance += 2.0*minor_cos*minor_sin*covariance*self.De_parameter_parameter_covariance(k, i, j)

        De_major_resolving_power = De_major_variance/(2.0*major_variance)
        De_minor_resolving_power = De_minor_variance/(2.0*minor_variance)

        if i < j:
            return De_major_resolving_power
        else:
            return De_minor_resolving_power

class TraceFileException(Exception):
    pass

def read_trace_file(filename):
    if not os.path.isfile(filename):
        raise TraceFileException('Trace file ' + filename + ' does not exist.')
    f = open(filename, 'rb')
    if f.read(1) == b'"':
        f.close()
        f = open(filename,'r')
    else:
        f.close()
        f = gzip.open(filename,'r')
    header = [name[1:-1] for name in f.readline()[:-1].split(',')]
    if header[-1] == 'LogLikelihood':
        raise TraceFileException('There are no log likelihood gradients in the trace file.')
    if 'LogLikelihood' not in header:
        raise TraceFileException('There is no log likelihood in the trace file.')
    loglikelihood_index = header.index('LogLikelihood')
    number_of_observables = 0
    for title in header[loglikelihood_index+1:]:
        if '*dLL/dsigma_' in title:
            break
        number_of_observables += 1
    number_of_parameters = loglikelihood_index - number_of_observables
    parameter_names = header[:number_of_parameters]
    observable_names = header[number_of_parameters:loglikelihood_index]
    correlations = Correlations(number_of_parameters, number_of_observables)

    for line in f:
        line = map(float, line.split(','))
        parameters = line[:number_of_parameters]
        observables = line[number_of_parameters:loglikelihood_index]
        loglikelihood = line[loglikelihood_index]
        value_gradient = line[loglikelihood_index+1:loglikelihood_index+1+number_of_observables]
        error_gradient = line[loglikelihood_index+1+number_of_observables:loglikelihood_index+1+2*number_of_observables]
        correlations.add_sample(parameters, observables, loglikelihood, value_gradient, error_gradient)

    return (correlations, observable_names, parameter_names)

def build_data_tables(correlations, observable_names, parameter_names):
    tables = {}

    #resolving power
    resolving_power = []
    for i, observable in enumerate(observable_names):
        row = []
        for j, parameter in enumerate(parameter_names):
            row.append(correlations.resolving_power(i, j, j))
        resolving_power.append(row)
    tables['resolving_power'] = DataTable(rows = observable_names, columns = parameter_names, \
                                          row_type = 'observable', column_type = 'parameter', \
                                          data = resolving_power)

    #2d resolving power
    tables['resolving_power_2d'] = []
    for k, observable in enumerate(observable_names):
        resolving_power_2d = []
        for i, parameter in enumerate(parameter_names):
            row = []
            for j in range(len(parameter_names)):
                row.append(correlations.resolving_power(k, i, j))
            resolving_power_2d.append(row)
        table = DataTable(rows = parameter_names, columns = parameter_names, \
                          row_type = 'parameter', column_type = 'parameter', \
                          data = resolving_power_2d)
        tables['resolving_power_2d'].append(table)

    #parameter response
    parameter_response = []
    for k, observable in enumerate(observable_names):
        row = []
        for i, parameter in enumerate(parameter_names):
            row.append(correlations.parameter_response_to_value(k, i))
        parameter_response.append(row)
    tables['parameter_response'] = DataTable(rows = observable_names, columns = parameter_names, \
                                             row_type = 'observable', column_type = 'parameter', \
                                             data = parameter_response)
    #scaled parameter response
    scaled_parameter_response = []
    for k, observable in enumerate(observable_names):
        row = []
        for i, parameter in enumerate(parameter_names):
            row.append(parameter_response[k][i])
            row[i] *= correlations.observable_standard_deviation(i)
        scaled_parameter_response.append(row)
    tables['scaled_parameter_response'] = DataTable(rows = observable_names, columns = parameter_names, \
                                             row_type = 'observable', column_type = 'parameter', \
                                             data = scaled_parameter_response)

    #parameter response to error
    parameter_response_to_error = []
    for k, observable in enumerate(observable_names):
        row = []
        for i, parameter in enumerate(parameter_names):
            row.append(correlations.parameter_response_to_error(k, i))
        parameter_response_to_error.append(row)
    tables['parameter_response_to_error'] = DataTable(rows = observable_names, columns = parameter_names, \
                                                      row_type = 'observable', column_type = 'parameter', \
                                                      data = parameter_response_to_error)

    #loglikelihood response
    loglikelihood_response = []
    for k, observable in enumerate(observable_names):
        loglikelihood_response.append(correlations.loglikelihood_response_to_value(k))
    tables['loglikelihood_response'] = DataTable(rows = ['dLL/d_observable'], columns = observable_names, \
                                                      row_type = 'other', column_type = 'observable', \
                                                      data = [loglikelihood_response])

    #scaled loglikelihood response
    scaled_loglikelihood_response = []
    for i in range(len(loglikelihood_response)):
        scaled_loglikelihood_response.append(loglikelihood_response[i])
        scaled_loglikelihood_response[i] *= correlations.observable_standard_deviation(i)
    tables['scaled_loglikelihood_response'] = DataTable(rows = ['sigma_observable*dLL/d_observable'], columns = observable_names, \
                                                      row_type = 'other', column_type = 'observable', \
                                                      data = [scaled_loglikelihood_response])
    return tables

def parse_parameters(stats_dir):
    parameters = {}
    with open(stats_dir + '/parameter_priors.dat', 'r') as f:
        for line in f:
            distribution, name, vmin, vmax = line.split()
            vmin, vmax = float(vmin), float(vmax)
            distribution = distribution.lower()
            if distribution != 'uniform':
                mean, sigma = vmin, vmax
                vmin, vmax = mean - sigma*N_SIGMA, mean + sigma*N_SIGMA
            parameters[name] = {'min' : vmin, 'max' : vmax}
    return parameters

def parse_observables(stats_dir):
    observables = {}
    with open(stats_dir + '/experimental_results.dat', 'r') as f:
        for line in f:
            name, mean, sigma = line.split()
            mean, sigma = float(mean), float(sigma)
            vmin, vmax = mean - sigma*N_SIGMA, mean + sigma*N_SIGMA
            observables[name] = {'min' : vmin, 'max' : vmax, 'mean' : mean, 'sigma' : sigma}
    return observables

if __name__ == '__main__':
    if len(sys.argv) != 4:
        print 'Usage: ' + sys.argv[0] + '/stats/directory/ trace_file.csv output.html'
        exit()
    try:
        correlations, observables, parameters = read_trace_file(sys.argv[2])
        tables = build_data_tables(correlations, observables, parameters)
        tables['observables'] = observables
        tables['parameters'] = parameters

        from jinja2 import Template
        template_path = os.path.dirname(os.path.realpath(__file__)) + '/overview_template.html'
        with open(template_path, 'r') as template_file:
            with open(sys.argv[3], 'w') as output_file:
                template = Template(template_file.read())
                output_file.write(template.render(**tables))

    except TraceFileException as e:
        print 'Fatal exception: ', e

