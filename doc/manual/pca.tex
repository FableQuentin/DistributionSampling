% !TEX root =  manual.tex
\section{PCA Analysis}

The emulator is dependent on the principal component analysis (PCA). PCA also provides insight of its own. The principal components show which linear combinations of observables are likely to be well constrained, and which combinations are uninteresting. Further, one can also estimate which parameters are driving specific principal components, at least within a linear picture.

PCA is performed by the following command,

\commandline{madai\_pca\_decompose stat1}

When running, \path{madai\_pca\_decompose} will read the parameters file \path{stat1/settings.dat}. The PCA analysis uses the following parameters:

\vspace*{-8pt}\begin{quote}
{\tt MODEL\_OUTPUT\_DIR "./model\_output"}\\
{\tt EXPERIMENTAL\_RESULTS\_DIR "./experimental\_results"}\\
{\tt N\_SAMPLE} $N$\\
$\vdots$
%{\tt PCA\_NCOMPONENTS} $M_Z$\\
%{\tt PCA\_LAMBDA\_PERCENTAGE} {\it percentage}\\
%{\tt USE\_EMULATOR\_ERROR false}\\
%{\tt MCMC\_BURN\_IN 100000}\\
%{\tt MCMC\_ITERATIONS 1000000}\\
%{\tt MCMC\_STEP\_SIZE 0.1}
\end{quote}\vspace*{-8pt}
The PCA analysis uses the experimental information only in calculating the error for comparing a model value to the experimental value, each of which has its own error.

\path{madai\_pca\_decompose} produces a file \path{stat1/PCADecomposition.dat}. This file holds the eigenvalues of $\Xi$ (the strength of the principal components) and the eigenvectors of $\Xi$ (which define the rotation of $\tilde{y}$ into the principal components, $z$. If $M$ experimental values are written down, the PCA analysis will determine and record the information recorded to rotate the vector $y_1\cdots y_M$ to $z_1\cdots z_M$. The emulator will in turn use only those components $z_1\cdots z_{M_Z}$ that correspond to the largest eigenvalues, $\lambda_i$, of the matrix $\Xi$. The number of components retained, $M_Z$, can be chosen during the emulator tuning or initialization.

The \path{madai\_pca\_decompose} file has the following format:
\begin{quote}
{\tt OUTPUT\_MEANS}\\
$M~~\leftarrow$ Number of observables.\\
$y_1$~~~$\leftarrow$ Means for observables averaged over the $N$ model runs.\\
$y_2$~~~$\swarrow$\\
$\vdots$\\
$y_M$\\
{\tt OUTPUT\_UNCERTAINTY\_SCALES}\\
$M~~\leftarrow$ Number of observables.\\
$\sigma_1$~~~$\leftarrow$ Uncertainties that incorporate both experimental and model uncertainties.\\
$\sigma_2$~~~$\leftarrow$ If different model runs have different $\sigma_i$, model error is averaged over runs.\\
$\sigma_3$~~~$\swarrow$\\
$\vdots$\\
$\sigma_M$\\
{\tt OUTPUT\_PCA\_EIGENVALUES}\\
$M$~~$\leftarrow$ Number of observables\\
$\lambda^{(M)}$~$\leftarrow$ Smallest eigenvalue of $\Xi_{ab}$ in Eq. \eqref{eq:Xidef}\\
$\lambda^{(M-1)}$~$\swarrow$\\
$\vdots$\\
$\lambda^{(1)}$~~~~$\leftarrow$ Largest eigenvalue\\
{\tt OUTPUT\_PCA\_EIGENVECTORS}\\
$M~~~~M$\\
\vspace*{-10pt}
\[
\hspace*{-130pt}
\begin{array}{cccc}
~~~~\xi_1^{\rm(M)}&\xi_1^{\rm(M-1)}&\cdots&\xi_1^{(1)}\\
~~~~\xi_2^{\rm(M)}&\xi_2^{\rm(M-1)}&\cdots&\xi_2^{(1)}\\
~~~~\vdots&\vdots&\ddots&\vdots\\
~~~~\xi_M^{\rm(M)}&\xi_M^{\rm(2)}&\cdots&\xi_M^{(1)}\\
\end{array} ~~~\leftarrow{\rm These~are~the~normalized~eigenvectors}
\]
{\tt END\_OF\_FILE}
\end{quote}

Before building an emulator, the experimental observables and the corresponding model values are rotated into a new basis. The rotation can be stated as,
\[
z_a=U_{ab}y_b,~~y_a=U^{-1}_{ab}z_b.
\]
As described in Eq. (\ref{eq:pcarotation}), the rotation matrix is comprised of eigenvectors of the matrix $\Xi_{ab}=\langle \tilde{y}_a\tilde{y}_b\rangle$, where $\tilde{y}_a\equiv (y_a-\langle y_a\rangle)/\sigma_a$, with $\langle y_a\rangle$ denoting $y_a({\bf x})$ averaged over model values calculated from the parameters taken from the prior distribution.
